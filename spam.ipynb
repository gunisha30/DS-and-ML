{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "spam.ipynb",
      "provenance": [],
      "mount_file_id": "1g7XEPYqKdVCQ9a48qNqcjg6A77Qv2dZt",
      "authorship_tag": "ABX9TyN6h/xy4j2qIQN0oujMxX4r",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gunisha30/projects/blob/master/spam.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PSyZaXEZiUih",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import nltk\n",
        "import re\n",
        "import string\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "df = pd.read_csv(\"//content//drive//My Drive//spam.csv\")\n",
        "\n",
        "#frequency of ham and spam \n",
        "pd.crosstab(df['Label'],columns='freq')\n",
        "\n",
        "#stemming\n",
        "ps = nltk.PorterStemmer()\n",
        "\n",
        "#list of stopwords\n",
        "stopwords = nltk.corpus.stopwords.words('english')\n",
        "\n",
        "#returns percentage of punctuations in text\n",
        "def punct_pc(text):\n",
        "    punct_count = sum([1 for char in text if char in string.punctuation])\n",
        "    return (punct_count/(len(text) - text.count(' ')))*100\n",
        "\n",
        "#creating two new features\n",
        "df['text_length'] = df['EmailText'].apply(lambda x : len(x)-x.count(' '))\n",
        "df['punct'] = df['EmailText'].apply(lambda x : punct_pc(x))\n",
        "\n",
        "#remove punctuations\n",
        "def clean_data(text):\n",
        "    punct = \"\".join([word.lower() for word in text if word not in string.punctuation])\n",
        "    splt = re.split('\\W+',punct)\n",
        "    txt = [ps.stem(word) for word in splt if word not in stopwords]\n",
        "    return txt\n",
        "\n",
        "x_train,x_test,y_train,y_test = train_test_split(df[['EmailText','text_length','punct']],df['Label'],test_size=0.2,random_state=123)\n",
        "\n",
        "#vectorising using count vectorizer\n",
        "vect=CountVectorizer(analyzer=clean_data)\n",
        "\n",
        "#fit- done on training data, tranform- applied to training and testing data\n",
        "vectfit = vect.fit(x_train['EmailText'])\n",
        "xtrainvect = vectfit.transform(x_train['EmailText'])\n",
        "xtestvect = vectfit.transform(x_test['EmailText'])\n",
        "xtrainvect= pd.concat([x_train[['text_length','punct']].reset_index(drop=True),pd.DataFrame(xtrainvect.toarray())],axis=1)\n",
        "xtestvect=pd.concat([x_test[['text_length','punct']].reset_index(drop=True),pd.DataFrame(xtestvect.toarray())],axis=1)\n",
        "\n",
        "#classifier- random forest\n",
        "rf = RandomForestClassifier(random_state=123,n_jobs=1)\n",
        "\n",
        "#get the list of hyperparameters, n_estimators=num of trees in the forest, max_features= num of features considered for splitting\n",
        "#print(rf.get_params())\n",
        "\n",
        "param = {'n_estimators' : [10,25,50,100,300], 'max_depth' : [10, 20, 50,100, None],'max_features' : [10,50,'auto']}\n",
        "grid = GridSearchCV(rf,param,cv=5,n_jobs=4)\n",
        "\n",
        "rf_grid_fit_2 = grid.fit(xtrainvect, y_train)\n",
        "#pd.DataFrame(rf_grid_fit_2.cv_results_).sort_values('mean_test_score',ascending=False)[0:10]\n",
        "\n",
        "#y_train.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V3N_-9b1kYMN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install nltk"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K6BTydIJ1_Zs",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 66
        },
        "outputId": "f3428b30-6c66-4dad-b0e5-f2f15b8c7264"
      },
      "source": [
        "nltk.download('stopwords')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ew1Q0nYz2AON",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install pyPDF2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "moB9R1uVXKDS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        },
        "outputId": "67e7ead3-a632-4c9e-8d8f-9b3b5579ddf1"
      },
      "source": [
        "import PyPDF2\n",
        "pdfFileObj = open('Gunisharesume.pdf', 'rb') \n",
        "pdfReader = PyPDF2.PdfFileReader(pdfFileObj) \n",
        "#print(pdfReader.numPages)\n",
        "pageObj = pdfReader.getPage(0) \n",
        "print(pageObj.extractText()) \n",
        "pdfFileObj.close()  "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tRjYXx4IAUul",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}